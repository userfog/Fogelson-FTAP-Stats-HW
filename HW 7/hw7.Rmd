---
title: "FTAP HW 7"
author: "Zachary Fogelson"
date: "July 21, 2015"
output: html_document
---

```{r, include = F}
library(gdata)
library(plyr)
library(knitr)
library(MASS)
library(glmnet)
library(pander)
library(xtable)
library(stargazer)
```

## Lasso Homework

### Problem 1

**1.**
```{r}
train <- read.csv("train.csv")
trainMeaningFull <-subset(train, select=-c(id, member_id))
trainMeaningFull <- trainMeaningFull[complete.cases(trainMeaningFull),]
```

```{r, results = "asis"}
stargazer(rbind(summary(train), sapply(train, sd)))
```

**2.**

a) 

```{r, eval = F}
ols <- lm(int_rate ~ ., trainMeaningFull)
olsSum <- summary(ols)
olsSum
```

Based on the OLS regression, all of the columns which are not IDs appear to be significant

b)

```{r, eval = F}
olsSum$r.squared
```

The baseline model has an $R^2$ of about .51. It is hard to judge if the baseline model is successful because, we do not have anything to compare it to. But using all of the available information we can explain 39% of the variance which is not terrible.

c)

```{r, eval = F}
plot(ols$fitted.values, ols$residuals)
```

d)

```{r, eval = F}
rmse <- mean((ols$residuals)^2)
rmse
```

e)

```{r,  eval = F}
test <- read.csv("test.csv")
test <- subset(test, select=-c(id, member_id))
test <- test[complete.cases(test),]
```

```{r, eval = F}
predOLS <- predict(ols,test, interval = "none")
rmseOLS <- mean((test$int_rate - predOLS)^2)
cat("RMSE OLS: ", rmseOLS)
```


**3.**

```{r,  eval = F}
interactDF <- as.data.frame(model.matrix(~(.)^2,subset(trainMeaningFull, select=-c(int_rate))))
interactTest <- as.data.frame(model.matrix(~(.)^2,subset(test, select=-c(int_rate))))
```

a) 
```{r, eval = F}
smallest = lm(trainMeaningFull$int_rate ~ 1, interactDF)
biggest = as.formula(lm(trainMeaningFull$int_rate ~ ., interactDF))
stepForwardAIC <- step(smallest, scope = biggest, k = 2, direction ="forward", trace = F)
stepForwardBIC <- step(smallest, scope = biggest, k = log(length(trainMeaningFull$int_rate)), direction ="forward", trace = F)
cat("R^2 AIC: ", summary(stepForwardAIC)$r.squared, "R^2 BIC: ", summary(stepForwardBIC)$r.squared)
```


b)
```{r,  eval = F}
cat("AIC: ", AIC(stepForwardAIC), "BIC: ", BIC(stepForwardBIC))
```

c)
```{r, eval = F}
rmseAIC <- mean((stepForwardAIC$residuals)^2)
rmseBIC <- mean((stepForwardBIC$residuals)^2)
cat("RMSE AIC: ", rmseAIC, "RMSE BIC: ", rmseBIC)
```

d)
```{r, eval = F}
predAIC <- predict(stepForwardAIC,interactTest, interval = "none")
predBIC <- predict(stepForwardBIC,interactTest, interval = "none")
rmseAIC <- mean((test$int_rate - predAIC)^2)
rmseBIC <- mean((test$int_rate - predBIC)^2)
cat("RMSE AIC: ", rmseAIC, "RMSE BIC: ", rmseBIC)
```


**4.**
a)
```{r, eval = F}
biggest = lm(trainMeaningFull$int_rate ~ ., interactDF)
smallest = as.formula(lm(trainMeaningFull$int_rate ~ 1, interactDF))
stepBackwardAIC <- step(biggest, scope = smallest, k = 2, direction ="backward", trace = F)
stepBackwardBIC <- step(biggest, scope = smallest, k = log(length(trainMeaningFull$int_rate)), direction ="backward", trace = F)
cat("R^2 AIC: ", summary(stepBackwardAIC)$r.squared, "R^2 BIC: ", summary(stepBackwardBIC)$r.squared)
```

b)

```{r, eval = F}
cat("AIC: ", AIC(stepBackwardAIC), "BIC: ", BIC(stepBackwardBIC))
```

c)
```{r, eval = F}
rmseBackAIC <- mean((stepBackwardAIC$residuals)^2)
rmseBackBIC <- mean((stepBackwardBIC$residuals)^2)
cat("RMSE AIC: ", rmseBackAIC, "RMSE BIC: ", rmseBackBIC)
```

d)
```{r, eval = F}
predBackAIC <- predict(stepBackwardAIC,interactTest, interval = "none")
predBackBIC <- predict(stepBackwardBIC,interactTest, interval = "none")
rmseBackAIC <- mean((test$int_rate - predBackAIC)^2)
rmseBackBIC <- mean((test$int_rate - predBackBIC)^2)
cat("RMSE AIC: ", rmseBackAIC, "RMSE BIC: ", rmseBackBIC)
```

**5.**

a)
```{r,  eval = F}
model=as.formula(paste("~", paste(names(interactDF)[-1], collapse= "+")))
x=model.matrix(model,interactDF);
lassoFit <- glmnet(x,trainMeaningFull$int_rate)
outLasso <- predict(lassoFit, newx = as.matrix(interactDF), s=.0001)
cat("R^2: ", var(outLasso)/var(ols$residuals+ols$fitted.values))
coef(lassoFit, s=.0001)
```

The $R^2$ is calculated based on the $R^2$ calculation in lasso_class; however, prof. Russell did mention that there is no $R^2$ for the lasso, so I printed the coefficients of the lambda model instead. 

b) 

```{r,  eval = F}
insamplePred <- predict(lassoFit, newx = as.matrix(interactDF), s=.0001)
lassoRMSE <- mean((insamplePred - trainMeaningFull$int_rate)^2)
cat("RMSE Lasso: ", lassoRMSE)
```

c)
```{r, eval = F}
predLasso <- predict(lassoFit,newx = as.matrix(interactTest), s=.0001)
lassoRMSE <- mean((test$int_rate - predLasso)^2)
cat("RMSE Lasso: ", lassoRMSE)
```

**6.**

a)
```{r,  eval = F}
lassoCVFit <- cv.glmnet(x,trainMeaningFull$int_rate, nfolds = 10)
lassoCVFit$lambda.min
```

b)
```{r,  eval = F}
insampleCVPred <- predict(lassoCVFit, newx = as.matrix(interactDF), s="lambda.min")
lassoCVRMSE <- mean((insampleCVPred - trainMeaningFull$int_rate)^2)
cat("RMSE CVLasso: ", lassoCVRMSE)
```

c)
```{r, eval = F}
predCVLasso <- predict(lassoCVFit,newx = as.matrix(interactTest), s="lambda.min")
lassoCVRMSE <- mean((test$int_rate - predCVLasso)^2)
cat("RMSE CVLasso: ", lassoCVRMSE)
```


## Logit Homework

```{r,  eval = F}
trades <- read.csv("detailed_trades_est.csv")
trades <- subset(trades, trades$PCHANGE0==0| trades$PCHANGE0==2) 
```

1)

```{r,  eval = F}
logit <- glm(PCHANGE~LASK+LBID+RETURN+SIGN_VOL, family=binomial, data=trades)
cat("Lasso? AIC: ", AIC(logit), "Lasso BIC: ", BIC(logit))
```


2)

```{r,  eval = F}
interactions <- subset(trades,select = -c(PCHANGE,PCHANGE0))
interactions <- as.data.frame(model.matrix(~(.)^2, interactions))
```

3)

```{r,  eval = F}
trades.pca <- prcomp(interactions)
myModel <- lm(trades$PCHANGE~trades.pca$x[,1]+trades.pca$x[,2]+trades.pca$x[,3])
```

4)
```{r,  eval = F}
error <- trades$PCHANGE-(as.numeric(myModel$fitted.values > .5))
mean(abs(error))
```
